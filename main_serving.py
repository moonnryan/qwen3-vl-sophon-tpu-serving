import time
import os
import sys
import json
import tempfile
from fastapi import Depends
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Header
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any, Union
import uvicorn
import asyncio
from contextlib import asynccontextmanager
import logging
import argparse
import traceback
import numpy as np
import requests
from concurrent.futures import ThreadPoolExecutor
import threading
import mimetypes

# ========== å‘½ä»¤è¡Œå‚æ•°è§£æ ==========
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Qwen3-VL TPUæ¨ç†æœåŠ¡")
    
    # æ ¸å¿ƒå‚æ•°ï¼ˆæŒ‡å®šæ¨¡å‹ç›®å½•ï¼‰
    parser.add_argument(
        "-m", "--model_dir", 
        default="./models/qwen3vl_2b",
        help="æ¨¡å‹ç›®å½•è·¯å¾„ (é»˜è®¤: ./models/qwen3vl_2b)"
    )
    
    # å¹¶å‘æ§åˆ¶å‚æ•°
    parser.add_argument(
        "-c", "--max_concurrent", 
        type=int, 
        default=10,
        help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 10)"
    )
    
    # æ—¥å¿—çº§åˆ«
    parser.add_argument(
        "-l", "--log_level", 
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )
    
    # TPUè®¾å¤‡ID
    parser.add_argument(
        "-d", "--devid", 
        type=int, 
        default=0,
        help="TPUè®¾å¤‡ID (é»˜è®¤: 0)"
    )
    
    # è§†é¢‘é‡‡æ ·æ¯”ä¾‹
    parser.add_argument(
        "-v", "--video_ratio", 
        type=float, 
        default=0.5,
        help="è§†é¢‘é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.5)"
    )
    
    # ç«¯å£å·
    parser.add_argument(
        "-p", "--port", 
        type=int, 
        default=8899,
        help="æœåŠ¡ç«¯å£å· (é»˜è®¤: 8899)"
    )
    
    # API Keyç›¸å…³å‚æ•°
    parser.add_argument(
        "--api-key", 
        type=str, 
        default="abc@123",
        help="APIè®¿é—®å¯†é’¥ï¼ˆå¯é€‰ï¼‰ï¼Œè‹¥è®¾ç½®åˆ™æ‰€æœ‰å—ä¿æŠ¤æ¥å£å¿…é¡»æºå¸¦è¯¥å¯†é’¥è®¿é—®"
    )
    parser.add_argument(
        "--api-key-header", 
        type=str, 
        default="Authorization",
        help="ä¼ é€’API Keyçš„HTTPè¯·æ±‚å¤´åç§°ï¼ˆé»˜è®¤: Authorizationï¼‰"
    )
    parser.add_argument(
        "--api-key-prefix", 
        type=str, 
        default="Bearer",
        help="API Keyçš„å‰ç¼€ï¼ˆé»˜è®¤: Bearerï¼‰ï¼Œæ ¼å¼ä¸ºã€Œå‰ç¼€ + ç©ºæ ¼ + å¯†é’¥ã€"
    )
    
    return parser.parse_args()

# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parse_args()

# ========== æ—¥å¿—é…ç½®ï¼ˆåŸºäºå‘½ä»¤è¡Œå‚æ•°ï¼‰ ==========
logging.basicConfig(
    level=getattr(logging, args.log_level),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ========== å…¨å±€é…ç½®ï¼ˆåŸºäºå‘½ä»¤è¡Œå‚æ•°ï¼‰ ==========
# å¹¶å‘æ§åˆ¶é…ç½®
MAX_CONCURRENT_REQUESTS = args.max_concurrent
EXECUTOR = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS)
REQUEST_LOCK = asyncio.Lock()

# æ–°å¢ï¼šAPI Keyå…¨å±€é…ç½®
API_CONFIG = {
    "enabled": args.api_key is not None,  # æ˜¯å¦å¯ç”¨API Keyè®¤è¯
    "api_key": args.api_key,              # æ ¸å¿ƒAPIå¯†é’¥
    "header_name": args.api_key_header,   # HTTPè¯·æ±‚å¤´åç§°
    "prefix": args.api_key_prefix         # API Keyå‰ç¼€
}

# è‡ªåŠ¨æ‹¼æ¥æ¨¡å‹è·¯å¾„å’Œé…ç½®è·¯å¾„
def find_bmodel_file(model_dir):
    """åœ¨æ¨¡å‹ç›®å½•ä¸­æŸ¥æ‰¾.bmodelæ–‡ä»¶"""
    for file in os.listdir(model_dir):
        if file.endswith(".bmodel"):
            return os.path.join(model_dir, file)
    raise FileNotFoundError(f"åœ¨ç›®å½• {model_dir} ä¸­æœªæ‰¾åˆ°.bmodelæ–‡ä»¶")

# æ¨¡å‹å…¨å±€é…ç½®ï¼ˆåŸºäºå‘½ä»¤è¡Œå‚æ•°è‡ªåŠ¨ç”Ÿæˆï¼‰
MODEL_CONFIG = {
    "model_path": find_bmodel_file(args.model_dir),
    "config_path": os.path.join(args.model_dir, "config"),
    "devid": args.devid,
    "video_ratio": args.video_ratio,
    "do_sample": False,
    "log_level": args.log_level
}

# æ¯ä¸ªçº¿ç¨‹å­˜å‚¨ç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹ï¼ˆçº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼‰
THREAD_LOCAL = threading.local()

def create_model_args():
    """åˆ›å»ºæ¨¡å‹å‚æ•°"""
    args = argparse.Namespace()
    args.model_path = MODEL_CONFIG["model_path"]
    args.config_path = MODEL_CONFIG["config_path"]
    args.devid = MODEL_CONFIG["devid"]
    args.video_ratio = MODEL_CONFIG["video_ratio"]
    return args

def get_thread_local_model():
    """è·å–å½“å‰çº¿ç¨‹çš„æ¨¡å‹å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    if not hasattr(THREAD_LOCAL, "model_instance"):
        try:
            logger.info(f"çº¿ç¨‹ {threading.get_ident()} åˆå§‹åŒ–æ¨¡å‹å®ä¾‹...")
            from pipeline import Qwen3_VL  # å¯¼å…¥pipline.pyçš„æ¨¡å‹ç±»
            args = create_model_args()
            THREAD_LOCAL.model_instance = Qwen3_VL(args)
            logger.info(f"çº¿ç¨‹ {threading.get_ident()} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"çº¿ç¨‹ {threading.get_ident()} æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    return THREAD_LOCAL.model_instance

async def load_model_global():
    """é¢„åŠ è½½ç¬¬ä¸€ä¸ªçº¿ç¨‹çš„æ¨¡å‹ï¼ˆæœåŠ¡å¯åŠ¨æ—¶ï¼‰"""
    try:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(EXECUTOR, get_thread_local_model)
        logger.info("âœ… å…¨å±€æ¨¡å‹é¢„åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        logger.error(f"âŒ å…¨å±€æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {e}")
        logger.error(traceback.format_exc())

# æ–°å¢ï¼šAPI KeyéªŒè¯å·¥å…·å‡½æ•°
def validate_api_key(headers: Dict[str, str]) -> bool:
    """
    éªŒè¯API Keyæ˜¯å¦æœ‰æ•ˆ
    :param headers: HTTPè¯·æ±‚å¤´å­—å…¸
    :return: éªŒè¯é€šè¿‡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # è‹¥æœªå¯ç”¨API Keyè®¤è¯ï¼Œç›´æ¥è¿”å›é€šè¿‡
    if not API_CONFIG["enabled"]:
        return True
    
    # æå–è¯·æ±‚å¤´ä¸­çš„è®¤è¯ä¿¡æ¯
    auth_header = headers.get(API_CONFIG["header_name"], "")
    if not auth_header:
        return False
    
    # æ‹†åˆ†å‰ç¼€å’Œå¯†é’¥ï¼ˆæ”¯æŒå¤§å°å†™ä¸æ•æ„Ÿçš„å‰ç¼€åˆ¤æ–­ï¼‰
    parts = auth_header.split(" ", 1)
    if len(parts) != 2:
        return False
    
    prefix, provided_key = parts
    if prefix.lower() != API_CONFIG["prefix"].lower():
        return False
    
    # å¯¹æ¯”å¯†é’¥ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    return provided_key == API_CONFIG["api_key"]

# æ–°å¢ï¼šä¾èµ–æ³¨å…¥å¼API KeyéªŒè¯ï¼ˆé€‚ç”¨äºå•ä¸ªæ¥å£ç²¾ç»†åŒ–æ§åˆ¶ï¼‰
async def require_api_key(
    api_header: Optional[str] = Header(None, alias=args.api_key_header)
) -> None:
    """
    FastAPIä¾èµ–é¡¹ï¼šéªŒè¯API Keyï¼Œå¤±è´¥åˆ™æŠ›å‡º401å¼‚å¸¸
    :param api_header: ä»æŒ‡å®šHTTPå¤´ä¸­æå–çš„è®¤è¯ä¿¡æ¯
    """
    # è‹¥æœªå¯ç”¨API Keyè®¤è¯ï¼Œç›´æ¥è¿”å›
    if not API_CONFIG["enabled"]:
        return
    
    # éªŒè¯é€»è¾‘
    if not api_header:
        raise HTTPException(
            status_code=401,
            detail=f"ç¼ºå°‘å¿…è¦çš„ {API_CONFIG['header_name']} è¯·æ±‚å¤´",
            headers={"WWW-Authenticate": API_CONFIG["prefix"]}
        )
    
    parts = api_header.split(" ", 1)
    if len(parts) != 2 or parts[0].lower() != API_CONFIG["prefix"].lower():
        raise HTTPException(
            status_code=401,
            detail=f"æ— æ•ˆçš„è®¤è¯æ ¼å¼ï¼Œæ­£ç¡®æ ¼å¼ï¼š{API_CONFIG['prefix']} <ä½ çš„API Key>",
            headers={"WWW-Authenticate": API_CONFIG["prefix"]}
        )
    
    provided_key = parts[1]
    if provided_key != API_CONFIG["api_key"]:
        raise HTTPException(
            status_code=401,
            detail="æ— æ•ˆçš„API Keyï¼Œè®¿é—®è¢«æ‹’ç»",
            headers={"WWW-Authenticate": API_CONFIG["prefix"]}
        )

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPIç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹
    await load_model_global()
    yield
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    EXECUTOR.shutdown(wait=True)
    logger.info("âœ… æœåŠ¡å·²å…³é—­ï¼Œèµ„æºæ¸…ç†å®Œæˆ")

app = FastAPI(
    title="Qwen3-VL TPUæ¨ç†æœåŠ¡",
    version="2.2.0",
    description="åŸºäºç®—èƒ½SE7ç›’å­çš„Qwen3-VLè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡ï¼ˆæ”¯æŒå¤šå¹¶å‘+æœ¬åœ°åª’ä½“æ–‡ä»¶+API Keyè®¤è¯ï¼‰",
    lifespan=lifespan
)

# ========== æ•°æ®æ¨¡å‹å®šä¹‰ ==========
class ChatMessage(BaseModel):
    role: str
    content: str | List[Dict[str, Any]]  # æ”¯æŒå­—ç¬¦ä¸²æˆ–å¤šæ¨¡æ€å†…å®¹

class ChatCompletionRequest(BaseModel):
    model: str = "qwen3-vl-instruct"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048
    stream: Optional[bool] = False

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Dict[str, int]

# ========== å·¥å…·å‡½æ•° ==========
def save_base64_image(base64_str: str) -> str:
    """ä¿å­˜base64å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶"""
    import base64
    import io
    try:
        if ',' in base64_str:
            base64_str = base64_str.split(',', 1)[1]
        image_data = base64.b64decode(base64_str)
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as f:
            f.write(image_data)
            return f.name
    except Exception as e:
        logger.error(f"ä¿å­˜base64å›¾ç‰‡å¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„base64å›¾ç‰‡æ•°æ®: {str(e)}")

def download_media_from_url(url: str) -> tuple[str, str]:
    """ä»URLä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡/è§†é¢‘ï¼‰åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›(æ–‡ä»¶è·¯å¾„, åª’ä½“ç±»å‹)"""
    try:
        logger.info(f"æ­£åœ¨ä»URLä¸‹è½½åª’ä½“: {url}")
        response = requests.get(url, timeout=15, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()

        # è¯†åˆ«åª’ä½“ç±»å‹
        content_type = response.headers.get('Content-Type', '')
        if content_type.startswith('image/'):
            media_type = "image"
            suffix = '.jpg' if 'jpeg' in content_type or 'jpg' in content_type else '.png'
        elif content_type.startswith('video/'):
            media_type = "video"
            suffix = '.mp4' if 'mp4' in content_type else '.avi'
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„åª’ä½“ç±»å‹: {content_type}")

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as f:
            f.write(response.content)
            logger.info(f"åª’ä½“å·²ä¸‹è½½å¹¶ä¿å­˜åˆ°: {f.name}")
            return f.name, media_type
    except requests.exceptions.Timeout:
        raise HTTPException(status_code=408, detail="ä¸‹è½½åª’ä½“è¶…æ—¶ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®")
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=400, detail=f"æ— æ³•ä¸‹è½½åª’ä½“: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½åª’ä½“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

def load_local_media(file_path: str) -> tuple[str, str]:
    """åŠ è½½æœ¬åœ°åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡/è§†é¢‘ï¼‰ï¼Œè¿”å›(æ–‡ä»¶è·¯å¾„, åª’ä½“ç±»å‹)"""
    try:
        # å¤„ç†file://åè®®
        if file_path.startswith("file://"):
            file_path = file_path[7:]
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        file_path = os.path.abspath(file_path)
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æƒé™
        if not os.access(file_path, os.R_OK):
            raise HTTPException(status_code=403, detail=f"æ— è¯»å–æƒé™: {file_path}")
        
        # è¯†åˆ«åª’ä½“ç±»å‹ï¼ˆä¼˜å…ˆmimetypesï¼Œå…œåº•æ‰©å±•åï¼‰
        content_type, _ = mimetypes.guess_type(file_path)
        if not content_type:
            ext = os.path.splitext(file_path)[1].lower()
            ext_map = {
                # å›¾ç‰‡
                '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png',
                '.bmp': 'image/bmp', '.gif': 'image/gif', '.webp': 'image/webp',
                # è§†é¢‘
                '.mp4': 'video/mp4', '.avi': 'video/x-msvideo', '.mov': 'video/quicktime',
                '.mkv': 'video/x-matroska', '.flv': 'video/x-flv', '.wmv': 'video/x-ms-wmv'
            }
            if ext not in ext_map:
                raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å: {ext} (æ–‡ä»¶: {file_path})")
            content_type = ext_map[ext]
        
        # ç¡®å®šåª’ä½“ç±»å‹
        if content_type.startswith('image/'):
            media_type = "image"
        elif content_type.startswith('video/'):
            media_type = "video"
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æœ¬åœ°åª’ä½“ç±»å‹: {content_type} (æ–‡ä»¶: {file_path})")
        
        logger.info(f"æˆåŠŸåŠ è½½æœ¬åœ°åª’ä½“: {file_path} (ç±»å‹: {media_type})")
        return file_path, media_type
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åŠ è½½æœ¬åœ°åª’ä½“å¤±è´¥: {str(e)} (æ–‡ä»¶: {file_path})")

def extract_content_and_media(messages: List[ChatMessage]) -> tuple[str, Optional[str], str]:
    """
    ä»OpenAIæ ¼å¼çš„æ¶ˆæ¯ä¸­æå–æ–‡æœ¬ã€åª’ä½“è·¯å¾„ã€åª’ä½“ç±»å‹
    æ”¯æŒï¼š1.æœ¬åœ°è·¯å¾„(file:///ç»å¯¹è·¯å¾„/ç›¸å¯¹è·¯å¾„) 2.Base64 3.è¿œç¨‹URL
    è¿”å›: (text_content, media_path, media_type)
    """
    system_prompt = ""
    text_parts = []
    media_path = None
    media_type = "text"

    for msg in messages:
        if msg.role == "system":
            if isinstance(msg.content, str):
                system_prompt = msg.content
            continue

        if msg.role == "user":
            if isinstance(msg.content, str):
                text_parts.append(msg.content)
            elif isinstance(msg.content, list):
                for item in msg.content:
                    if not isinstance(item, dict):
                        continue
                    item_type = item.get("type", "")

                    if item_type == "text":
                        text_parts.append(item.get("text", ""))
                    elif item_type == "image_url":  # å¤ç”¨è¯¥å­—æ®µæ”¯æŒæ‰€æœ‰åª’ä½“ç±»å‹
                        if media_path:  # åªå¤„ç†ç¬¬ä¸€ä¸ªåª’ä½“æ–‡ä»¶
                            continue
                        image_url_data = item.get("image_url", {})
                        url = image_url_data.get("url", "") if isinstance(image_url_data, dict) else image_url_data
                        
                        # 1. æœ¬åœ°æ–‡ä»¶ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
                        if url.startswith(("file://", "/", "./", "../")):
                            media_path, media_type = load_local_media(url)
                        # 2. Base64å›¾ç‰‡
                        elif url.startswith("data:image"):
                            media_path = save_base64_image(url)
                            media_type = "image"
                        # 3. è¿œç¨‹URLï¼ˆå›¾ç‰‡/è§†é¢‘ï¼‰
                        elif url.startswith(("http://", "https://")):
                            media_path, media_type = download_media_from_url(url)

    # ç»„åˆæ–‡æœ¬å†…å®¹
    user_content = " ".join(text_parts).strip()
    if not user_content and media_path:
        # æ— æ–‡æœ¬æ—¶é»˜è®¤ç”Ÿæˆæè¿°æŒ‡ä»¤
        user_content = "è¯·è¯¦ç»†æè¿°è¿™ä¸ªåª’ä½“æ–‡ä»¶çš„å†…å®¹ã€‚"
    if system_prompt:
        logger.warning(f"System promptæš‚æ—¶ç¦ç”¨: {system_prompt}")
    return user_content, media_path, media_type

# ========== æ ¸å¿ƒæ¨ç†å‡½æ•°ï¼ˆåŒæ­¥ï¼Œè¿è¡Œåœ¨çº¿ç¨‹æ± ï¼‰ ==========
def process_inference_sync(prompt: str, media_path: Optional[str], media_type: str, stream: bool = False):
    """
    åŒæ­¥æ¨ç†å‡½æ•°ï¼ˆè¿è¡Œåœ¨çº¿ç¨‹æ± ï¼‰
    è¿”å›: éæµå¼è¿”å›æ–‡æœ¬ï¼Œæµå¼è¿”å›ç”Ÿæˆå™¨
    """
    try:
        # è·å–å½“å‰çº¿ç¨‹çš„æ¨¡å‹å®ä¾‹
        model = get_thread_local_model()
        # é‡ç½®æ¨¡å‹å†å²ï¼ˆå…³é”®ï¼šè¯·æ±‚éš”ç¦»ï¼‰
        model.model.clear_history()
        model.history_max_posid = 0
        model.input_str = prompt

        # æ„å»ºæ¶ˆæ¯
        if media_type == "text":
            messages = model.text_message()
        elif media_type == "image":
            messages = model.image_message(media_path)
        elif media_type == "video":
            messages = model.video_message(media_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åª’ä½“ç±»å‹: {media_type}")

        # å¤„ç†è¾“å…¥
        inputs = model.process(messages, media_type)
        token_len = inputs.input_ids.numel()
        if token_len > model.model.MAX_INPUT_LENGTH:
            raise ValueError(f"è¾“å…¥é•¿åº¦è¶…é™: {token_len} > {model.model.MAX_INPUT_LENGTH}")

        # åµŒå…¥å±‚
        model.model.forward_embed(inputs.input_ids)

        # è§†è§‰å¤„ç†
        position_ids = None
        if media_type == "image":
            model.vit_process_image(inputs)
            position_ids = model.get_rope_index(inputs.input_ids, inputs.image_grid_thw, model.ID_IMAGE_PAD)
            model.max_posid = int(position_ids.max())
        elif media_type == "video":
            model.vit_process_video(inputs)
            position_ids = model.get_rope_index(inputs.input_ids, inputs.video_grid_thw, model.ID_VIDEO_PAD)
            model.max_posid = int(position_ids.max())
        else:
            position_ids = np.array([list(range(token_len))]*3, dtype=np.int32)
            model.max_posid = token_len - 1

        # é¢„å¡«å……
        prefill_token = model.forward_prefill(position_ids)  # é‡å‘½åå˜é‡é¿å…ä½œç”¨åŸŸå†²çª

        if stream:
            # æµå¼ç”Ÿæˆï¼ˆè¿”å›ç”Ÿæˆå™¨ï¼‰
            def generate_stream():
                chunk_id = f"chatcmpl-{int(time.time())}"
                full_word_tokens = []
                token = prefill_token  # æ˜¾å¼èµ‹å€¼ï¼Œé¿å…æœªå®šä¹‰

                try:
                    # ç¬¬ä¸€ä¸ªtoken
                    if token is not None and token not in [model.ID_IM_END, model.ID_END] and token != model.tokenizer.eos_token_id:
                        full_word_tokens.append(token)
                        word = model.tokenizer.decode(full_word_tokens, skip_special_tokens=True)
                        if "ï¿½" not in word:
                            chunk = {
                                "id": chunk_id,
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": "qwen3-vl-instruct",
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": word},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                            full_word_tokens = []

                    # åç»­token
                    for step in range(2047):  # é™åˆ¶æœ€å¤§é•¿åº¦
                        if model.model.history_length >= model.model.SEQLEN:
                            break
                        model.max_posid += 1
                        pos_ids = np.array([model.max_posid]*3, dtype=np.int32)
                        token = model.model.forward_next(pos_ids)

                        if token in [model.ID_IM_END, model.ID_END]:
                            # ç»“æŸæ ‡è®°
                            chunk = {
                                "id": chunk_id,
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": "qwen3-vl-instruct",
                                "choices": [{
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": "stop"
                                }]
                            }
                            yield f"data: {json.dumps(chunk)}\n\n"
                            yield "data: [DONE]\n\n"
                            break

                        if token is None:
                            continue

                        full_word_tokens.append(token)
                        word = model.tokenizer.decode(full_word_tokens, skip_special_tokens=True)
                        if "ï¿½" not in word:
                            chunk = {
                                "id": chunk_id,
                                "object": "chat.completion.chunk",
                                "created": int(time.time()),
                                "model": "qwen3-vl-instruct",
                                "choices": [{
                                    "index": 0,
                                    "delta": {"content": word},
                                    "finish_reason": None
                                }]
                            }
                            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                            full_word_tokens = []

                except Exception as e:
                    # æµå¼å¼‚å¸¸å¤„ç†ï¼šè¿”å›é”™è¯¯ä¿¡æ¯
                    logger.error(f"æµå¼ç”Ÿæˆé”™è¯¯: {e}")
                    error_chunk = {
                        "error": {
                            "message": f"æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}",
                            "type": "stream_error"
                        }
                    }
                    yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    # æœ¬åœ°æ–‡ä»¶ä¸åˆ é™¤ï¼Œä»…æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆBase64/URLä¸‹è½½çš„ï¼‰
                    if media_path and (media_path.startswith(tempfile.gettempdir()) or "tmp" in media_path):
                        try:
                            os.unlink(media_path)
                        except:
                            pass
                return

            return generate_stream()
        else:
            # éæµå¼ç”Ÿæˆï¼ˆåŸæœ‰é€»è¾‘ï¼Œè¡¥å……tokenç©ºå€¼æ£€æŸ¥ï¼‰
            full_word_tokens = []
            response_text = ""
            token = prefill_token  # æ˜¾å¼èµ‹å€¼
            
            # ç¬¬ä¸€ä¸ªtoken
            if token is not None and token not in [model.ID_IM_END, model.ID_END] and token != model.tokenizer.eos_token_id:
                full_word_tokens.append(token)
                word = model.tokenizer.decode(full_word_tokens, skip_special_tokens=True)
                if "ï¿½" not in word:
                    response_text += word
                    full_word_tokens = []

            # åç»­token
            for step in range(2047):
                if model.model.history_length >= model.model.SEQLEN:
                    break
                model.max_posid += 1
                pos_ids = np.array([model.max_posid]*3, dtype=np.int32)
                token = model.model.forward_next(pos_ids)

                if token in [model.ID_IM_END, model.ID_END]:
                    break

                if token is None:
                    continue

                full_word_tokens.append(token)
                word = model.tokenizer.decode(full_word_tokens, skip_special_tokens=True)
                if "ï¿½" not in word:
                    response_text += word
                    full_word_tokens = []

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆæœ¬åœ°æ–‡ä»¶ä¸åˆ é™¤ï¼‰
            if media_path and (media_path.startswith(tempfile.gettempdir()) or "tmp" in media_path):
                try:
                    os.unlink(media_path)
                except:
                    pass

            return response_text.strip() or "æŠ±æ­‰ï¼Œæ¨¡å‹æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå›å¤ã€‚"
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if media_path and (media_path.startswith(tempfile.gettempdir()) or "tmp" in media_path):
            try:
                os.unlink(media_path)
            except:
                pass
        logger.error(f"æ¨ç†å¤±è´¥: {e}")
        raise

# ========== APIæ¥å£ ==========
@app.get("/")
async def root():
    """æœåŠ¡ä¸»é¡µ"""
    api_info = {
        "api_key_enabled": API_CONFIG["enabled"],
        "api_key_header": API_CONFIG["header_name"],
        "api_key_format": f"{API_CONFIG['prefix']} <your-api-key>" if API_CONFIG['enabled'] else "æœªå¯ç”¨"
    }
    
    return {
        "message": "Qwen3-VL TPUæ¨ç†æœåŠ¡è¿è¡Œä¸­ï¼ˆæ”¯æŒå¤šå¹¶å‘+æœ¬åœ°åª’ä½“æ–‡ä»¶+API Keyè®¤è¯ï¼‰",
        "model": "qwen3-vl-instruct",
        "device": "BM1684X TPU",
        "max_concurrent": MAX_CONCURRENT_REQUESTS,
        "timestamp": int(time.time()),
        "version": "2.2.0",
        "api_config": api_info,
        "model_config": {
            "model_dir": args.model_dir,
            "model_path": MODEL_CONFIG["model_path"],
            "devid": args.devid,
            "video_ratio": args.video_ratio
        },
        "supported_media": {
            "local_file": "æ”¯æŒfile:///ç»å¯¹è·¯å¾„ã€/ç»å¯¹è·¯å¾„ã€./ç›¸å¯¹è·¯å¾„ã€../ä¸Šçº§è·¯å¾„",
            "image_format": "jpg/jpeg/png/bmp/gif/webp",
            "video_format": "mp4/avi/mov/mkv/flv/wmv"
        },
        "endpoints": {
            "chat": "/v1/chat/completions",
            "media": "/v1/media/describe",
            "health": "/health",
            "models": "/v1/models",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(EXECUTOR, get_thread_local_model)
        status = "healthy"
        details = "æ¨¡å‹å·²åŠ è½½ä¸”è¿è¡Œæ­£å¸¸"
    except Exception as e:
        status = "unhealthy"
        details = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    return {
        "status": status,
        "details": details,
        "model": "qwen3-vl-instruct",
        "model_dir": args.model_dir,
        "max_concurrent": MAX_CONCURRENT_REQUESTS,
        "api_key_enabled": API_CONFIG["enabled"],
        "timestamp": int(time.time()),
        "version": "2.2.0"
    }

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    # æ–°å¢ï¼šä¾èµ–æ³¨å…¥éªŒè¯API Key
    _: None = Depends(require_api_key)
):
    """
    OpenAIå…¼å®¹çš„èŠå¤©å¯¹è¯æ¥å£ï¼ˆæ”¯æŒå¤šå¹¶å‘+æœ¬åœ°å›¾ç‰‡/è§†é¢‘+API Keyè®¤è¯ï¼‰
    æ”¯æŒï¼š1.æœ¬åœ°åª’ä½“æ–‡ä»¶ 2.Base64å›¾ç‰‡ 3.è¿œç¨‹URLåª’ä½“ 4.çº¯æ–‡æœ¬
    """
    if not request.messages:
        raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦ä¸€æ¡æ¶ˆæ¯")

    # æå–å†…å®¹å’Œåª’ä½“
    user_message, media_path, media_type = extract_content_and_media(request.messages)
    if not user_message and not media_path:
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯æˆ–åª’ä½“æ–‡ä»¶")

    # å¹¶å‘æ§åˆ¶ï¼šè·å–é”
    async with REQUEST_LOCK:
        try:
            loop = asyncio.get_running_loop()
            if request.stream:
                # æµå¼å“åº”
                logger.info(f"æµå¼å¤„ç†è¯·æ±‚ï¼ˆ{media_type}ï¼‰: {user_message[:50]}...")
                stream_generator = await loop.run_in_executor(
                    EXECUTOR, process_inference_sync,
                    user_message, media_path, media_type, True
                )

                # è‡ªå®šä¹‰å¼‚æ­¥è¿­ä»£å™¨åŒ…è£…å™¨
                async def async_stream_wrapper():
                    try:
                        for chunk in stream_generator:
                            yield chunk
                            await asyncio.sleep(0.001)
                    except Exception as e:
                        logger.error(f"æµå¼è¿­ä»£é”™è¯¯: {e}")
                        error_chunk = f"data: {json.dumps({'error': str(e)})}\n\n"
                        yield error_chunk
                        yield "data: [DONE]\n\n"

                return StreamingResponse(
                    async_stream_wrapper(),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "X-Accel-Buffering": "no"
                    }
                )
            else:
                # éæµå¼å“åº”
                start_time = time.time()
                logger.info(f"å¤„ç†è¯·æ±‚ï¼ˆ{media_type}ï¼‰: {user_message[:50]}...")
                response_text = await loop.run_in_executor(
                    EXECUTOR, process_inference_sync,
                    user_message, media_path, media_type, False
                )

                # æ„å»ºå“åº”
                response_id = f"chatcmpl-{int(time.time())}"
                created_time = int(time.time())
                choice = ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(role="assistant", content=response_text),
                    finish_reason="stop"
                )
                usage = {
                    "prompt_tokens": len(user_message.split()),
                    "completion_tokens": len(response_text.split()),
                    "total_tokens": len(user_message.split()) + len(response_text.split())
                }

                return ChatCompletionResponse(
                    id=response_id,
                    created=created_time,
                    model=request.model,
                    choices=[choice],
                    usage=usage
                )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"èŠå¤©æ¨ç†é”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.post("/v1/media/describe")
async def describe_media(
    file: UploadFile = File(...),
    prompt: str = Form(default="è¯·ç®€å•æè¿°è¿™ä¸ªåª’ä½“æ–‡ä»¶çš„å†…å®¹ã€‚"),
    stream: bool = Form(default=False),
    # ä¾èµ–æ³¨å…¥éªŒè¯API Key
    _: None = Depends(require_api_key)
):
    """åª’ä½“æè¿°æ¥å£ï¼ˆæ”¯æŒå›¾ç‰‡/è§†é¢‘ï¼Œæµå¼/éæµå¼è¾“å‡º+API Keyè®¤è¯ï¼‰"""
    start_time = time.time()
    temp_path = None

    try:
        # 1. å¿«é€Ÿæ ¡éªŒæ–‡ä»¶ç±»å‹ï¼ˆç®€åŒ–åˆ¤æ–­é€»è¾‘ï¼‰
        media_type = None
        if file.content_type:
            if file.content_type.startswith('image/'):
                media_type = "image"
            elif file.content_type.startswith('video/'):
                media_type = "video"
        
        # å…œåº•ï¼šé€šè¿‡æ–‡ä»¶æ‰©å±•ååˆ¤æ–­ï¼ˆé˜²æ­¢content_typeä¸å‡†ç¡®ï¼‰
        if not media_type:
            ext = os.path.splitext(file.filename)[1].lower()
            image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
            video_exts = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
            if ext in image_exts:
                media_type = "image"
            elif ext in video_exts:
                media_type = "video"
        
        if not media_type:
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file.content_type or 'æœªçŸ¥'}ï¼Œä»…æ”¯æŒå›¾ç‰‡/è§†é¢‘"
            )

        # 2. é«˜æ•ˆä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼ˆå‡å°‘IOæ“ä½œï¼‰
        suffix = os.path.splitext(file.filename)[1] or ('.jpg' if media_type == 'image' else '.mp4')
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            # ç›´æ¥å†™å…¥ï¼Œé¿å…äºŒæ¬¡è¯»å–
            tmp.write(await file.read())
            temp_path = tmp.name

        logger.info(f"å¼€å§‹å¤„ç†{media_type}æè¿°è¯·æ±‚ï¼š{file.filename} | prompt: {prompt[:30]}...")

        # 3. å¹¶å‘æ§åˆ¶ + æ¨ç†ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
        async with REQUEST_LOCK:
            loop = asyncio.get_running_loop()
            
            if stream:
                # æµå¼å“åº”ï¼ˆç®€åŒ–ç”Ÿæˆå™¨åŒ…è£…ï¼‰
                stream_generator = await loop.run_in_executor(
                    EXECUTOR, process_inference_sync,
                    prompt, temp_path, media_type, True
                )

                async def async_stream_wrapper():
                    try:
                        for chunk in stream_generator:
                            yield chunk
                            await asyncio.sleep(0.001)  # é˜²æ­¢é˜»å¡äº‹ä»¶å¾ªç¯
                    except Exception as e:
                        err_msg = f"æµå¼ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
                        logger.error(err_msg)
                        yield f"data: {json.dumps({'error': err_msg}, ensure_ascii=False)}\n\n"
                        yield "data: [DONE]\n\n"

                return StreamingResponse(
                    async_stream_wrapper(),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
                        "Connection": "keep-alive"
                    }
                )
            else:
                # éæµå¼å“åº”ï¼ˆç®€åŒ–è¿”å›ç»“æ„ï¼‰
                description = await loop.run_in_executor(
                    EXECUTOR, process_inference_sync,
                    prompt, temp_path, media_type, False
                )
                
                # è®¡ç®—å¤„ç†è€—æ—¶
                processing_time = round(time.time() - start_time, 2)
                
                return {
                    "status": "success",
                    "description": description,
                    "metadata": {
                        "filename": file.filename,
                        "media_type": media_type,
                        "prompt": prompt,
                        "processing_time_seconds": processing_time,
                        "model": "qwen3-vl-instruct",
                        "model_dir": args.model_dir
                    }
                }

    except HTTPException:
        raise
    except Exception as e:
        err_detail = f"å¤„ç†{media_type or 'åª’ä½“'}æ–‡ä»¶å¤±è´¥ï¼š{str(e)}"
        logger.error(f"{err_detail} | æ–‡ä»¶ï¼š{file.filename}")
        raise HTTPException(status_code=500, detail=err_detail)
    finally:
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except:
                pass

@app.get("/v1/models")
async def list_models(
    # æ–°å¢ï¼šä¾èµ–æ³¨å…¥éªŒè¯API Key
    _: None = Depends(require_api_key)
):
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            {
                "id": "qwen3-vl-instruct",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "SE7-Box-TPU",
                "permission": [],
                "root": "qwen3-vl-instruct",
                "parent": None,
                "description": f"Qwen3-VLæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼ˆæ¨¡å‹ç›®å½•ï¼š{args.model_dir}ï¼‰ï¼Œåœ¨ç®—èƒ½BM1684X TPUä¸Šè¿è¡Œ"
            }
        ]
    }

@app.get("/v1/models/{model_id}")
async def get_model(
    model_id: str,
    # æ–°å¢ï¼šä¾èµ–æ³¨å…¥éªŒè¯API Key
    _: None = Depends(require_api_key)
):
    """è·å–æŒ‡å®šæ¨¡å‹ä¿¡æ¯"""
    if model_id != "qwen3-vl-instruct":
        raise HTTPException(status_code=404, detail="æ¨¡å‹æœªæ‰¾åˆ°")
    return {
        "id": "qwen3-vl-instruct",
        "object": "model",
        "created": int(time.time()),
        "owned_by": "SE7-Box-TPU",
        "model_config": {
            "model_dir": args.model_dir,
            "model_path": MODEL_CONFIG["model_path"],
            "devid": args.devid,
            "max_concurrent": MAX_CONCURRENT_REQUESTS
        },
        "api_config": API_CONFIG,
        "description": f"Qwen3-VLæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼ˆæ¨¡å‹ç›®å½•ï¼š{args.model_dir}ï¼‰ï¼Œåœ¨ç®—èƒ½BM1684X TPUä¸Šè¿è¡Œ"
    }

# ========== å¯åŠ¨é…ç½® ==========
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨Qwen3-VL TPUæ¨ç†æœåŠ¡ï¼ˆæ”¯æŒå¤šå¹¶å‘+æœ¬åœ°åª’ä½“æ–‡ä»¶+API Keyè®¤è¯ï¼‰...")
    print(f"ğŸ¯ æ¨¡å‹ç›®å½•: {args.model_dir}")
    print(f"ğŸ¯ æ¨¡å‹æ–‡ä»¶: {MODEL_CONFIG['model_path']}")
    print(f"ğŸ”§ è®¾å¤‡ID: {args.devid}")
    print(f"âš¡ æœ€å¤§å¹¶å‘æ•°: {MAX_CONCURRENT_REQUESTS}")
    print(f"ğŸ“ æ—¥å¿—çº§åˆ«: {args.log_level}")
    print(f"ğŸ¬ è§†é¢‘é‡‡æ ·æ¯”ä¾‹: {args.video_ratio}")
    print(f"ğŸ“– APIæ–‡æ¡£: http://0.0.0.0:{args.port}/docs")
    print(f"ğŸ’¬ èŠå¤©æ¥å£: http://0.0.0.0:{args.port}/v1/chat/completions")
    print(f"ğŸ–¼ï¸  åª’ä½“æè¿°: http://0.0.0.0:{args.port}/v1/media/describe")
    print(f"ğŸ“ æ”¯æŒæœ¬åœ°åª’ä½“: ç»å¯¹è·¯å¾„(/xxx/xxx.jpg)ã€ç›¸å¯¹è·¯å¾„(./xxx.jpg)ã€file://åè®®")
    
    # æ‰“å°API Keyé…ç½®ä¿¡æ¯
    if API_CONFIG["enabled"]:
        print(f"ğŸ”’ API Keyè®¤è¯å·²å¯ç”¨: è¯·æ±‚å¤´ {API_CONFIG['header_name']} = {API_CONFIG['prefix']} {API_CONFIG['api_key'][:4]}****{API_CONFIG['api_key'][-4:]}")
    else:
        print(f"âš ï¸ API Keyè®¤è¯æœªå¯ç”¨ï¼Œç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ --api-key å‚æ•°é…ç½®è®¿é—®å¯†é’¥")

    # å¯åŠ¨uvicorn
    uvicorn.run(
        "main_serving:app",
        host="0.0.0.0",
        port=args.port,
        reload=False,
        log_level=args.log_level.lower(),
        workers=1,
        loop="uvloop",
        http="httptools"
    )